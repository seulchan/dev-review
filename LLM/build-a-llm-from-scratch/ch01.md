# Chapter 1. Understanding Large Language Models

## Table of Contents
- [Pretrainingê³¼ Fine-tuning](#pretrainingê³¼-fine-tuning)
- [Transformer êµ¬ì¡°](#transformer-êµ¬ì¡°)
- [GPT ì•„í‚¤í…ì²˜](#gpt-ì•„í‚¤í…ì²˜)
- [LLM ê°œë°œì˜ ì„¸ ë‹¨ê³„](#llm-ê°œë°œì˜-ì„¸-ë‹¨ê³„)
- [ë‚´ê°€ ì´í•´í•œ í•µì‹¬](#ë‚´ê°€-ì´í•´í•œ-í•µì‹¬)
- [ğŸ“– ì›ë¬¸ ë°œì·Œ](#-ì›ë¬¸-ë°œì·Œ)

---

LLM(Large Language Model)ì€ **ì¸ê°„ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ìƒì„±í•˜ëŠ” ì‹ ê²½ë§ ê¸°ë°˜ ëª¨ë¸**ì´ë‹¤. ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì§•ì€ ë°©ëŒ€í•œ í…ìŠ¤íŠ¸ë¥¼ í•™ìŠµí•˜ì—¬ ë¬¸ë§¥ì„ ì´í•´í•˜ê³ , ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìˆë‹¤ëŠ” ì ì´ë‹¤.

---

## Pretrainingê³¼ Fine-tuning

![pre-training and fine-tuning](./images/1-3.png)

LLMì€ ë¨¼ì € **ì‚¬ì „í•™ìŠµ(Pretraining)** ë‹¨ê³„ë¥¼ ê±°ì¹œë‹¤.  
ì´ ë‹¨ê³„ì—ì„œëŠ” ë°©ëŒ€í•œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµí•œë‹¤.  
- *"raw text"* = Labelì´ ì—†ëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸ ë°ì´í„°  
- ê²°ê³¼ë¬¼ = **pretrained LLM** ë˜ëŠ” **foundation model**

ê·¸ë‹¤ìŒì—ëŠ” **íŒŒì¸íŠœë‹(Fine-tuning)** ë‹¨ê³„ê°€ ë’¤ë”°ë¥¸ë‹¤. Labelì´ í¬í•¨ëœ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ íŠ¹ì • íƒœìŠ¤í¬ì— ë§ê²Œ ì¡°ì •í•œë‹¤.

### Fine-tuningì˜ ë‘ ê°€ì§€ ëŒ€í‘œ ë°©ì‹
- **Instruction fine-tuning**: ì§ˆë¬¸ê³¼ ë‹µë³€ ìŒìœ¼ë¡œ í•™ìŠµ (ì˜ˆ: ë²ˆì—­ ìš”ì²­ â†’ ì •ë‹µ ë²ˆì—­)
- **Classification fine-tuning**: í…ìŠ¤íŠ¸ì™€ ë ˆì´ë¸” ìŒìœ¼ë¡œ í•™ìŠµ (ì˜ˆ: ì´ë©”ì¼ â†’ â€œspam / not spamâ€)

---

## Transformer êµ¬ì¡°

![original transformer](./images/1-4.png)

í˜„ëŒ€ LLMì˜ í•µì‹¬ì€ **Transformer ì•„í‚¤í…ì²˜**(Vaswani et al., 2017, *Attention Is All You Need*)ì´ë‹¤.  
íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ë‘ ë¶€ë¶„ìœ¼ë¡œ êµ¬ì„±ëœë‹¤.

- **Encoder**: ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (ë¬¸ë§¥ ì •ë³´ ì¸ì½”ë”©)  
- **Decoder**: ì´ ë²¡í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶œë ¥ í…ìŠ¤íŠ¸ ìƒì„±  

í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ì€ **Self-Attention**ì´ë‹¤.  
ì´ëŠ” ì…ë ¥ ì‹œí€€ìŠ¤ ë‚´ ë‹¨ì–´ë“¤ ê°„ì˜ ì¤‘ìš”ë„ë¥¼ ë™ì ìœ¼ë¡œ ê³„ì‚°í•˜ì—¬ **ì¥ê±°ë¦¬ ì˜ì¡´ì„±**ê³¼ **ë¬¸ë§¥ì  ê´€ê³„**ë¥¼ ì˜ í•™ìŠµí•˜ê²Œ í•œë‹¤.

---

## GPT ì•„í‚¤í…ì²˜

![GPT](./images/1-8.png)

GPTëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ì˜ **Decoder ë¶€ë¶„ë§Œ ì‚¬ìš©í•˜ëŠ” êµ¬ì¡°**ì´ë‹¤.  
ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ìˆœì°¨ì ìœ¼ë¡œ ë‹¨ì–´ë¥¼ ìƒì„±í•˜ë©°,  
í…ìŠ¤íŠ¸ ìƒì„± ë° ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì— ìµœì í™”ë˜ì–´ ìˆë‹¤.  
ì¦‰, í•œ ë‹¨ì–´ì”© ì˜ˆì¸¡í•˜ë©´ì„œ ë¬¸ì¥ì„ ì™„ì„±í•´ ë‚˜ê°„ë‹¤.

---

## LLM ê°œë°œì˜ ì„¸ ë‹¨ê³„

![LLM stages](./images/1-9.png)

LLMì„ ë§Œë“¤ê³  í™œìš©í•˜ëŠ” ê³¼ì •ì€ í¬ê²Œ ì„¸ ë‹¨ê³„ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤.

1. **ì•„í‚¤í…ì²˜ êµ¬í˜„ & ë°ì´í„° ì¤€ë¹„**  
   - Transformer ê¸°ë°˜ LLM êµ¬ì¡° ì •ì˜  
   - í•™ìŠµìš© í…ìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬  
2. **ì‚¬ì „í•™ìŠµ (Pretraining)**  
   - ë°©ëŒ€í•œ ë°ì´í„°ë¡œ ì–¸ì–´ ì¼ë°˜ ëŠ¥ë ¥ì„ ìŠµë“  
   - ê²°ê³¼ë¬¼: Foundation Model  
3. **íŒŒì¸íŠœë‹ (Fine-tuning)**  
   - íŠ¹ì • ì—…ë¬´ì— ë§ê²Œ ì¡°ì •  
   - ê°œì¸ ë¹„ì„œí˜• LLM, ë¶„ë¥˜ê¸° ë“± ë‹¤ì–‘í•œ í˜•íƒœë¡œ ë°œì „  

---

## ë‚´ê°€ ì´í•´í•œ í•µì‹¬

- **Pretraining** = ì–¸ì–´ì˜ ì¼ë°˜ì  íŒ¨í„´ í•™ìŠµ  
- **Fine-tuning** = íŠ¹ì • íƒœìŠ¤í¬ ë§ì¶¤ í•™ìŠµ  
- **Transformer** = Encoder + Decoder êµ¬ì¡°  
- **GPT** = Decoderë§Œ í™œìš© â†’ í…ìŠ¤íŠ¸ ìƒì„±ì— íŠ¹í™”  
- **LLM ê°œë°œ ë‹¨ê³„** = ì•„í‚¤í…ì²˜ êµ¬í˜„ â†’ ì‚¬ì „í•™ìŠµ â†’ íŒŒì¸íŠœë‹

---

## ğŸ“– ì›ë¬¸ ë°œì·Œ

<details>
<summary>ì›ë¬¸ ë³´ê¸°</summary>
An LLM is a neural network designed to understand, generate, and respond to human-like text. 

The first step in creating an LLM is to train it on a large corpus of text data, sometimes referred to as raw text. Here, *"raw"* refers to the fact that this data is just regular text without any labeling information.

This first training stage of an LLM is also known as **pretraining**, creating an initial pretrained LLM, often called a *base* or *foundation model*. 

After obtaining a pretrained LLM by training on large text datasets, where the LLM is trained to predict the next word in the text, we can further train the LLM on labeled data, also known as **fine-tuning**.

The two most popular categories of fine-tuning LLMs are **instruction fine-tuning** and **classification fine-tuning**. In instruction fine-tuning, the labeled dataset consists of instruction and answer pairs, such as a query to translate a text accompanied by the correctly translated text. In classification fine-tuning, the labeled dataset consists of texts and associated class labelsâ€”for example, emails associated with â€œspamâ€ and â€œnot spamâ€ labels.

Most modern LLMs rely on the transformer architecture, which is a deep neural network architecture introduced in the 2017 paper â€œAttention Is All You Needâ€ (https://arxiv.org/abs/1706.03762).

The transformer architecture consists of two submodules: an *encoder* and a *decoder*. The encoder module processes the input text and encodes it into a series of numerical representations or vectors that capture the contextual information of the input. Then, the decoder module takes these encoded vectors and generates the output text. 

A key component of transformers and LLMs is the **self-attention mechanism**, which allows the model to weigh the importance of different words or tokens in a sequence relative to each other. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data, enhancing its ability to generate coherent and contextually relevant output.

The GPT architecture employs only the decoder portion of the original transformer. It is designed for unidirectional, left-to-right processing, making it well suited for text generation and next-word prediction tasks to generate text in an iterative fashion, one word at a time.

The three main stages of coding an LLM are implementing the LLM architecture and data preparation process (stage 1), pretraining an LLM to create a foundation model (stage 2), and fine-tuning the foundation model to become a personal assistant or text classifier (stage 3).
</details>