# Chapter 1. An Introduction to Large Language Models

## Table of Contents
- [Embeddings](#embeddings)
- [Transformer êµ¬ì¡°](#transformer-êµ¬ì¡°)
- [BERTì™€ Encoder-only ëª¨ë¸](#bertì™€-encoder-only-ëª¨ë¸)
- [GPTì™€ Decoder-only ëª¨ë¸](#gptì™€-decoder-only-ëª¨ë¸)
- [LLM ê°œë°œ ê³¼ì •](#llm-ê°œë°œ-ê³¼ì •)
- [ë‚´ê°€ ì´í•´í•œ í•µì‹¬](#ë‚´ê°€-ì´í•´í•œ-í•µì‹¬)
- [ğŸ“– ì›ë¬¸ ë°œì·Œ](#-ì›ë¬¸-ë°œì·Œ)

---

## Embeddings
![different embeddings](./images/1-1.png)

- **ì„ë² ë”©(embedding)** = ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ í‘œí˜„í•´ ì˜ë¯¸ë¥¼ ì¡ì•„ë‚´ëŠ” ë°©ì‹  
- í…ìŠ¤íŠ¸ë¿ë§Œ ì•„ë‹ˆë¼ ì´ë¯¸ì§€, ìŒì„± ë“± ë‹¤ì–‘í•œ ì…ë ¥ì— ëŒ€í•´ ìƒì„± ê°€ëŠ¥  
- ì„ë² ë”©ì€ LLMì˜ **í‘œí˜„ í•™ìŠµ ê¸°ë°˜**ìœ¼ë¡œ, ë¬¸ë§¥ì  ì˜ë¯¸ë¥¼ ìˆ˜ì¹˜í™”í•˜ëŠ” í•µì‹¬ ë„êµ¬  

---

## Transformer êµ¬ì¡°
![transformer](./images/1-2.png)
- Transformer = **ì¸ì½”ë” ë¸”ë¡ + ë””ì½”ë” ë¸”ë¡**ì´ ì¸µì¸µì´ ìŒ“ì¸ êµ¬ì¡°  

![encoder](./images/1-3.png)
- **Encoder** = Self-Attention + Feedforward NN  
  - ì…ë ¥ì„ ë¬¸ë§¥ì  ë²¡í„° í‘œí˜„ìœ¼ë¡œ ë³€í™˜  

![decoder](./images/1-4.png)
- **Decoder** = Masked Self-Attention + Encoder Attention + Feedforward NN  
  - Masked Self-Attention: ì´ë¯¸ ìƒì„±ëœ ë‹¨ì–´ë“¤(Previously generated words) ê°„ì˜ ë¬¸ë§¥ì  ê´€ê³„ë¥¼ íŒŒì•…
  - Encoder Attention (Cross-Attention): ì¸ì½”ë” ì¶œë ¥(Transformer encoder Output)ê³¼ Masked Self-Attentionì˜ ê²°ê³¼ ê°„ì˜ ê´€ê³„ë¥¼ í†µí•´ ì…ë ¥ ë¬¸ì¥ì˜ ë¬¸ë§¥ì„ ì°¸ì¡°
  - ì´ ë‘ ê°€ì§€ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ í™œìš©í•˜ì—¬ ë‹¤ìŒì— ì˜¬ ë‹¨ì–´(Next generated word)ë¥¼ ì˜ˆì¸¡í•˜ê³  ìƒì„±

---

## BERTì™€ Encoder-only ëª¨ë¸
![BERT](./images/1-5.png)
- **BERT** = Encoder-only êµ¬ì¡°  

![MLM](./images/1-7.png)
- í›ˆë ¨ ë°©ë²• = **Masked Language Modeling (MLM)**  
  - ì…ë ¥ ë¬¸ì¥ì—ì„œ ì¼ë¶€ ë‹¨ì–´ë¥¼ ê°€ë ¤(mask) ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµ  
  - ê°€ë ¤ì§„ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ ë¬¸ì¥ì˜ ì™¼ìª½ê³¼ ì˜¤ë¥¸ìª½ ë¬¸ë§¥ì„ ëª¨ë‘ ì°¸ê³ í•˜ë¯€ë¡œ 'ì–‘ë°©í–¥(Bidirectional)' ë¬¸ë§¥ ì´í•´ ëŠ¥ë ¥ì´ ë›°ì–´ë‚¨


![BERT í•™ìŠµ ê³¼ì •](./images/1-6.png)
- íŠ¹ì§•:
  - í…ìŠ¤íŠ¸ **í‘œí˜„(representation)** í•™ìŠµì— ìµœì í™”  
  - ì£¼ë¡œ **Transfer Learning**ì— í™œìš© (ì‚¬ì „í•™ìŠµ í›„ íŠ¹ì • íƒœìŠ¤í¬ íŒŒì¸íŠœë‹)  
  - ìƒì„±(generation)ë³´ë‹¤ëŠ” ì˜ë¯¸ íŒŒì•…ì— ì§‘ì¤‘  

---

## GPTì™€ Decoder-only ëª¨ë¸
![gpt](./images/1-8.png)

- **GPT** = Decoder-only êµ¬ì¡°  
- ëª©ì : **ìƒì„±(generative tasks)**  
- í…ìŠ¤íŠ¸ë¥¼ ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ í•œ ë‹¨ì–´ì”© ì˜ˆì¸¡í•˜ë©° ìƒì„± (ìˆœë°©í–¥/ìê¸°íšŒê·€, Autoregressive ë°©ì‹)
- ì´ˆê¸° ëª¨ë¸(GPT-1)ì€ BERTì™€ ë‹¬ë¦¬ **ì–¸ì–´ ìƒì„±**ì— ìµœì í™”ë¨  
- ì´í›„ GPT-2, GPT-3, GPT-4ë¡œ í™•ì¥ â†’ ì˜¤ëŠ˜ë‚ ì˜ LLM íë¦„ì„ ì£¼ë„  

---

## LLM ê°œë°œ ê³¼ì •
![ml](./images/1-9.png)

ì „í†µ ML = íŠ¹ì • íƒœìŠ¤í¬ ì „ìš© ëª¨ë¸ í›ˆë ¨ (1-step).  
LLM = **2-step í•™ìŠµ ê³¼ì •**:

1. **Pretraining (ì‚¬ì „í•™ìŠµ)**  
   - ë°©ëŒ€í•œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì¼ë°˜ ì–¸ì–´ íŒ¨í„´ í•™ìŠµ  
   - ê²°ê³¼: *Foundation Model / Base Model*  
   - ì˜ˆ: Llama 2ëŠ” ì•½ 2ì¡° ê°œì˜ í† í°ìœ¼ë¡œ í•™ìŠµë¨  
   - ë§¤ìš° ë¹„ìš©ì´ í¬ê³  ëŒ€ê·œëª¨ ì»´í“¨íŒ… ìì›ì´ í•„ìš”  

2. **Fine-tuning (íŒŒì¸íŠœë‹)**  
   - ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì„ íŠ¹ì • íƒœìŠ¤í¬/í–‰ë™ìœ¼ë¡œ ì¡°ì •  
   - ì˜ˆ: ë¶„ë¥˜ê¸°(classifier), ì§€ì‹œ ë”°ë¥´ëŠ” ëª¨ë¸(instruction-following)  
   - Pretrainingì˜ ë¹„ìš©ì„ ì ˆì•½í•  ìˆ˜ ìˆëŠ” ì‹¤ìš©ì  ì ‘ê·¼  

â†’ Pretrained Model = Pretrainingë§Œ ê±°ì¹œ ëª¨ë¸ + Fine-tuned ëª¨ë¸ ëª¨ë‘ í¬í•¨  

---

## ë‚´ê°€ ì´í•´í•œ í•µì‹¬

- **Embeddings** = ë°ì´í„° ì˜ë¯¸ë¥¼ ë²¡í„°ë¡œ í‘œí˜„  
- **BERT** = Encoder-only, MLM ê¸°ë°˜ â†’ ì–‘ë°©í–¥ ë¬¸ë§¥ì„ í†µí•œ í‘œí˜„ í•™ìŠµ ìµœì í™”
- **GPT** = Decoder-only, ìê¸°íšŒê·€(Autoregressive) ë°©ì‹ â†’ í…ìŠ¤íŠ¸ ìƒì„± ìµœì í™”
- **Representation vs Generative Models**
  - Representation Models (BERTë¥˜): ì–¸ì–´ ì´í•´, ì„ë² ë”© ìƒì„±  
  - Generative Models (GPTë¥˜): í…ìŠ¤íŠ¸ ìƒì„±  
- **LLM í•™ìŠµ ê³¼ì •**
  - Pretraining = ì–¸ì–´ ì¼ë°˜ íŒ¨í„´ ìŠµë“  
  - Fine-tuning = íŠ¹ì • íƒœìŠ¤í¬ ìµœì í™”  

---

## ğŸ“– ì›ë¬¸ ë°œì·Œ

Embeddings are vector representations of data that attempt to capture its meaning. 

Embeddings can be created for different types of input.

The Transformer is a combination of stacked encoder and decoder blocks where the input flows through each encoder and decoder.

The encoder block in the Transformer consists of two parts, self-attention and a feedforward neural network.

The decoder has an additional attention layer that attends to the output of the encoder.

BERT(Bidirectional Encoder Representations from Transformers) is an encoder-only architecture that focuses on representing language. This means that it only uses the encoder and removes the decoder entirely.

Training these encoder stacks can be a difficult task that BERT approaches by adopting a technique called masked language modeling. This method masks a part of the input for the model to predict. This prediction task is difficult but allows BERT to create more accurate (intermediate) representations of the input.

This architecture and training procedure makes BERT and related architectures incredible at representing contextual language. BERT-like models are commonly used for transfer learning, which involves first pretraining it for language modeling and then fine-tuning it for a specific task. For instance, by training BERT on the entirety of Wikipedia, it learns to understand the semantic and contextual nature of text. Then, as shown in Figure 1-23, we can use that pretrained model to fine-tune it for a specific task, like text classification.

Throughout the book, we will refer to encoder-only models as representation models to differentiate them from decoder-only, which we refer to as generative models. Note that the main distinction does not lie between the underlying architecture and the way these models work. Representation models mainly focus on representing language, for instance, by creating embeddings, and typically do not generate text. In contrast, generative models focus primarily on generating text and typically are not trained to generate embeddings.

Similar to the encoder-only architecture of BERT, a decoder-only architecture was proposed in 2018 to target generative tasks. This architecture was called a Generative Pre-trained Transformer (GPT) for its generative capabilities (itâ€™s now known as GPT-1 to distinguish it from later versions). It stacks decoder blocks similar to the encoder-stacked architecture of BERT.

These generative decoder-only models, especially the â€œlargerâ€ models, are commonly referred to as large language models (LLMs). As we will discuss later in this chapter, the term LLM is not only reserved for generative models (decoder-only) but also representation models (encoder-only).

Traditional machine learning generally involves training a model for a specific task,
like classification. We consider this to be a one-step process.

Creating LLMs, in contrast, typically consists of at least two steps:
1) Language modeling
    
    The first step, called *pretraining*, takes the majority of computation and training time. An LLM is trained on a vast corpus of internet text allowing the model to learn grammar, context, and language patterns. This broad training phase is not yet directed toward specific tasks or applications beyond predicting the next word. The resulting model is often referred to as a *foundation model* or *base model*. These models generally do not follow instructions.

2) Fine-tuning

    The second step, *fine-tuning* or sometimes *post-training*, involves using the previously trained model and further training it on a narrower task. This allows the LLM to adapt to specific tasks or to exhibit desired behavior. For example, we could fine-tune a base model to perform well on a classification task or to follow instructions. It saves massive amounts of resources because the pretraining phase is quite costly and generally requires data and computing resources that are out of the reach of most people and organizations. For instance, Llama 2 has been trained on a dataset containing 2 trillion tokens. Imagine the compute necessary to create that model!
    
Any model that goes through the first step, *pretraining*, we consider a *pretrained model*, which also includes fine-tuned models.

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-3-mini-4k-instruct",
    device_map="mps",
    torch_dtype="auto",
    trust_remote_code=True,
)
tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")


# Create a pipeline
generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    return_full_text=False,
    max_new_tokens=500,
    do_sample=False
)

# The prompt (user input / query)
messages = [
    {"role": "user", "content": "Create a funny joke about chickens."}
]

# Generate output
output = generator(messages)
print(output[0]["generated_text"])
# Why did the chicken join the band? Because it had the drumsticks!
```