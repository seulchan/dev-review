# Week 3 â€” Open Source Foundations: Hugging Face, Pipelines, Tokenizers & Models

## ğŸ§­ ëª©ì°¨

1. [Pipeline â€” ê³ ìˆ˜ì¤€ ì¶”ë¡  API](#pipeline--ê³ ìˆ˜ì¤€-ì¶”ë¡ -api)
2. [Tokenizer â€” í…ìŠ¤íŠ¸ì™€ í† í°ì˜ ë³€í™˜](#tokenizer--í…ìŠ¤íŠ¸ì™€-í† í°ì˜-ë³€í™˜)
3. [ëª¨ë¸ â€” LLM ì‹¤í–‰ ë° ë¹„êµ](#ëª¨ë¸--llm-ì‹¤í–‰-ë°-ë¹„êµ)
4. [í•µì‹¬ ìš”ì•½](#í•µì‹¬-ìš”ì•½)
5. [ì½”ë“œ](#ì½”ë“œ)

---

## Pipeline â€” ê³ ìˆ˜ì¤€ ì¶”ë¡  API

* **ê°œë…:** ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ë‹¨ í•œ ì¤„ë¡œ ë¶ˆëŸ¬ì™€ ë‹¤ì–‘í•œ ë¨¸ì‹ ëŸ¬ë‹ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ì¸í„°í˜ì´ìŠ¤.

* **ê¸°ë³¸ êµ¬ì¡°:**

  ```python
  from transformers import pipeline
  nlp = pipeline("sentiment-analysis")
  nlp("I'm thrilled to learn LLM engineering!")
  ```

* **ìì£¼ ì“°ì´ëŠ” íŒŒì´í”„ë¼ì¸ ì¢…ë¥˜:**

  * `sentiment-analysis` (ê°ì • ë¶„ì„)
  * `text-classification`, `ner`, `qa` (ë¶„ë¥˜, ê°œì²´ëª… ì¸ì‹, ì§ˆì˜ì‘ë‹µ)
  * `summarization`, `translation` (ìš”ì•½, ë²ˆì—­)
  * `text-generation`, `image-generation`, `text-to-speech` (ìƒì„±í˜• ëª¨ë¸)

* **GPU í™œìš©:** `device="cuda"` ì˜µì…˜ìœ¼ë¡œ GPU ê°€ì† ê°€ëŠ¥.

* **í•µì‹¬ ì¥ì :** ë‹¤ì–‘í•œ ëª¨ë¸ì„ **í”ŒëŸ¬ê·¸ ì•¤ í”Œë ˆì´** í˜•íƒœë¡œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥.

---

## Tokenizer â€” í…ìŠ¤íŠ¸ì™€ í† í°ì˜ ë³€í™˜

* **ì •ì˜:** í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” **ìˆ«ì í† í°**ìœ¼ë¡œ ë³€í™˜í•˜ê³ , ë‹¤ì‹œ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ë¡œ ë³µì›.

* **ì£¼ìš” ë©”ì„œë“œ:**

  * `encode(text)` â†’ í† í° ID ë¦¬ìŠ¤íŠ¸
  * `decode(tokens)` â†’ í…ìŠ¤íŠ¸ ë³µì›

* **íŠ¹ìˆ˜ í† í°:**

  * `<s>` / `<bos>` : ë¬¸ì¥ì˜ ì‹œì‘
  * `</s>` / `<eos>` : ë¬¸ì¥ì˜ ë
  * `<pad>`, `<unk>`, `<system>`, `<user>`, `<assistant>`

* **Chat Template:**
  ëŒ€í™”í˜• ëª¨ë¸ì˜ ì‹œìŠ¤í…œÂ·ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì¼ì •í•œ í¬ë§·ìœ¼ë¡œ ì •ë¦¬

  ```python
  messages = [
      {"role": "system", "content": "You are helpful."},
      {"role": "user", "content": "Tell me a joke."}
  ]
  tokenizer.apply_chat_template(messages, tokenize=False)
  ```

* **ì£¼ì˜í•  ì :**
  ëª¨ë¸ë³„ë¡œ **í† í¬ë‚˜ì´ì €ê°€ ë‹¤ë¦„** (ì˜ˆ: LLaMA, Phi, Qwen ë“±).
  ë‹¤ë¥¸ í† í¬ë‚˜ì´ì €ë¥¼ ì„ì–´ ì“°ë©´ ì…ë ¥ í¬ë§·ì´ ê¹¨ì§.

---

## ëª¨ë¸ â€” LLM ì‹¤í–‰ ë° ë¹„êµ

* **ì •ì˜:** ì‹¤ì œ ì¶”ë¡ (ìƒì„±)ì„ ìˆ˜í–‰í•˜ëŠ” **Transformer ì‹ ê²½ë§**.

* **í´ë˜ìŠ¤:** `AutoModelForCausalLM.from_pretrained()`

* **ì–‘ìí™”(Quantization):**

  * ëª¨ë¸ì˜ ìˆ˜ì¹˜ ì •ë°€ë„ë¥¼ ì¤„ì—¬ (ì˜ˆ: 32bit â†’ 8bit â†’ 4bit)
    **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ ë¹„ìš© ì ˆê°**
  * ì•½ê°„ì˜ ì •í™•ë„ ì†ì‹¤ì´ ìˆì„ ìˆ˜ ìˆìŒ
  * `BitsAndBytesConfig`ë¡œ ê´€ë¦¬ (`bitsandbytes` ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)

---

## í•µì‹¬ ìš”ì•½

* **Pipeline**ì€ ì¶”ë¡ ì„ ê°„ì†Œí™”í•˜ê³ , **Tokenizer**ëŠ” ì–¸ì–´ êµ¬ì¡°ë¥¼ ì •ì˜í•˜ë©°, **Model**ì€ ì‹¤ì œ ì—°ì‚°ì„ ìˆ˜í–‰í•œë‹¤.
* **ì–‘ìí™”(Quantization)**ë¥¼ í†µí•´ ëŒ€í˜• ëª¨ë¸ë„ ì œí•œëœ í•˜ë“œì›¨ì–´ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•˜ë‹¤.

---

## ì½”ë“œ

<details>
<summary>ì½”ë“œ ë³´ê¸°</summary>

```python
# ============================================================
# ğŸ§  Week 3
# ============================================================

# (1) Pipeline ì˜ˆì‹œ
from transformers import pipeline
sentiment = pipeline("sentiment-analysis", device="cuda")
print(sentiment("I love open-source LLMs!"))

# ------------------------------------------------------------

# (2) Tokenizer ì˜ˆì‹œ
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B")
tokens = tokenizer.encode("Hello LLM Engineers!")
print(tokens, "â†’", tokenizer.decode(tokens))

# ------------------------------------------------------------

# (3) Chat Template
messages = [
    {"role": "system", "content": "You are helpful."},
    {"role": "user", "content": "Tell a data-science joke."}
]
print(tokenizer.apply_chat_template(messages, tokenize=False))

# ------------------------------------------------------------

# (4) ì–‘ìí™” ëª¨ë¸ ë¡œë”©
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
bnb_cfg = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True)
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Meta-Llama-3-8B",
    quantization_config=bnb_cfg,
    device_map="auto"
)

# ------------------------------------------------------------

# (5) ìƒì„± + ìŠ¤íŠ¸ë¦¬ë°
from transformers import TextStreamer
streamer = TextStreamer(tokenizer)
inputs = tokenizer.apply_chat_template(messages, return_tensors="pt").to("cuda")
model.generate(inputs, max_new_tokens=80, streamer=streamer)

# ============================================================
```

</details>
