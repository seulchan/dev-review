# Chapter 8. Image classification

## Table of Contents
- [Dense Layer vs Convolution Layer](#dense-layer-vs-convolution-layer)
- [í•©ì„±ê³± ì‹ ê²½ë§ (CNNs)](#í•©ì„±ê³±-ì‹ ê²½ë§-cnns)
- [íŒ¨ë”©ê³¼ ìŠ¤íŠ¸ë¼ì´ë“œ](#íŒ¨ë”©ê³¼-ìŠ¤íŠ¸ë¼ì´ë“œ)
- [í’€ë§(Pooling)](#í’€ë§pooling)
- [ê³¼ì í•©ê³¼ ë°ì´í„° ì¦ê°•](#ê³¼ì í•©ê³¼-ë°ì´í„°-ì¦ê°•)
- [ì „ì´í•™ìŠµê³¼ ì‚¬ì „í•™ìŠµ ëª¨ë¸](#ì „ì´í•™ìŠµê³¼-ì‚¬ì „í•™ìŠµ-ëª¨ë¸)
- [íŠ¹ì§• ì¶”ì¶œ vs íŒŒì¸íŠœë‹](#íŠ¹ì§•-ì¶”ì¶œ-vs-íŒŒì¸íŠœë‹)
- [ë¶€ë¶„ íŒŒì¸íŠœë‹](#ë¶€ë¶„-íŒŒì¸íŠœë‹)
- [ë‚´ê°€ ì´í•´í•œ í•µì‹¬](#ë‚´ê°€-ì´í•´í•œ-í•µì‹¬)
- [ì½”ë“œ](#ì½”ë“œ)
- [ğŸ“– ì›ë¬¸ ë°œì·Œ](#-ì›ë¬¸-ë°œì·Œ)

---

## Dense Layer vs Convolution Layer
![conv](./images/08-01.png)

- **Dense Layer**: ì…ë ¥ ì „ì²´ë¥¼ ëŒ€ìƒìœ¼ë¡œ **ì „ì—­ íŒ¨í„´** í•™ìŠµ  
- **Convolution Layer**: ì‘ì€ 2D ìœˆë„ìš°ì—ì„œ **êµ­ì†Œ íŒ¨í„´** í•™ìŠµ  

â†’ CNNì€ ì´ë¯¸ì§€ì˜ ê³µê°„ì  êµ¬ì¡°ë¥¼ í¬ì°©í•˜ëŠ” ë° ê°•ì ì´ ìˆìŒ  

---

## í•©ì„±ê³± ì‹ ê²½ë§ (CNNs)
![how-convolution-works](./images/08-02.png)

- **í‰í–‰ ì´ë™ ë¶ˆë³€ì„±(translation invariance)**: íŒ¨í„´ì´ ìœ„ì¹˜ê°€ ë‹¬ë¼ì ¸ë„ ì¸ì‹  
- **ê³„ì¸µì  íŠ¹ì§• í•™ìŠµ**: ë‹¨ìˆœí•œ íŠ¹ì§•(ì—ì§€, ì§ˆê°) â†’ ë³µì¡í•œ íŠ¹ì§•(í˜•íƒœ, ê°ì²´)  
- **Feature map**: ê° í•„í„°ê°€ í™œì„±í™”ëœ ê³µê°„ì  ë¶„í¬ë¥¼ ë‚˜íƒ€ëƒ„  

---

## íŒ¨ë”©ê³¼ ìŠ¤íŠ¸ë¼ì´ë“œ
- **íŒ¨ë”© (padding)**  
  - `"valid"`: íŒ¨ë”© ì—†ìŒ â†’ ì¶œë ¥ í¬ê¸° ê°ì†Œ  
  - `"same"`: íŒ¨ë”© ì¶”ê°€ â†’ ì¶œë ¥ í¬ê¸° = ì…ë ¥ í¬ê¸° ìœ ì§€  
- **ìŠ¤íŠ¸ë¼ì´ë“œ (stride)**  
  - ìœˆë„ìš° ì´ë™ ê°„ê²©  
  - ê¸°ë³¸ê°’ = 1  
  - stride=2 â†’ ë„ˆë¹„Â·ë†’ì´ ì ˆë°˜ìœ¼ë¡œ ë‹¤ìš´ìƒ˜í”Œë§  

---

## í’€ë§(Pooling)
- **Max Pooling**  
  - 2Ã—2 ìœˆë„ìš° + stride=2 â†’ feature map í¬ê¸° ì ˆë°˜ ê°ì†Œ  
  - êµ­ì†Œ íŒ¨ì¹˜ì—ì„œ ê°€ì¥ ê°•í•œ í™œì„±ê°’ë§Œ ì„ íƒ â†’ íŠ¹ì§• ì¡´ì¬ ì—¬ë¶€ë¥¼ ì˜ ë°˜ì˜  
- í‰ê·  í’€ë§ì´ë‚˜ strided convë³´ë‹¤ **íŠ¹ì§• ë³´ì¡´ë ¥ì´ ìš°ìˆ˜**  

---

## ê³¼ì í•©ê³¼ ë°ì´í„° ì¦ê°•
- **ê³¼ì í•©** = ë°ì´í„°ê°€ ì ì„ ë•Œ ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ë¥¼ ì™¸ì›Œì„œ ì¼ë°˜í™” ëª»í•¨  
- **ë°ì´í„° ì¦ê°•** = ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ë‹¤ì–‘í•œ ëœë¤ ë³€í™˜ â†’ ìƒˆë¡œìš´ ìƒ˜í”Œ ìƒì„±  
  - ëª¨ë¸ì´ ë™ì¼í•œ ì´ë¯¸ì§€ë¥¼ ë‘ ë²ˆ ë³´ì§€ ì•Šê²Œ í•˜ì—¬ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ  
- í•˜ì§€ë§Œ ì¦ê°•ë§Œìœ¼ë¡œëŠ” ë¶€ì¡± â†’ **Dropout** ì¶”ê°€ í•„ìš”  

---

## ì „ì´í•™ìŠµê³¼ ì‚¬ì „í•™ìŠµ ëª¨ë¸
- **ì‚¬ì „í•™ìŠµ ëª¨ë¸(pretrained model)** = ëŒ€ê·œëª¨ ë°ì´í„°ì…‹(ImageNet ë“±)ìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸  
- í•™ìŠµëœ **ê³„ì¸µì  íŠ¹ì§• í‘œí˜„**ì„ ìƒˆë¡œìš´ ê³¼ì œì— ì¬í™œìš© ê°€ëŠ¥  
- ì¥ì : ì‘ì€ ë°ì´í„°ì…‹ì—ì„œë„ ì¢‹ì€ ì„±ëŠ¥ ë°œíœ˜  
- ì˜ˆ: ImageNet(ë™ë¬¼, ì‚¬ë¬¼) â†’ ê°€êµ¬ ë¶„ë¥˜ë¡œ ì¬ì‚¬ìš©  

---

## íŠ¹ì§• ì¶”ì¶œ vs íŒŒì¸íŠœë‹
![feature-extraction](./images/08-03.png)

- **íŠ¹ì§• ì¶”ì¶œ(feature extraction)**  
  - ì‚¬ì „í•™ìŠµëœ conv base â†’ feature map ì¶”ì¶œ â†’ ìƒˆë¡œìš´ classifier í•™ìŠµ  
  - ì¥ì : ë¹ ë¥´ê³  ê³„ì‚° íš¨ìœ¨ì   
  - ë‹¨ì : ë°ì´í„° ì¦ê°• ë¶ˆê°€  

- **ì—”ë“œ íˆ¬ ì—”ë“œ í•™ìŠµ**  
  - conv base + Dense layer ì „ì²´ í•™ìŠµ  
  - ì¥ì : ë°ì´í„° ì¦ê°• ê°€ëŠ¥  
  - ë‹¨ì : ê³„ì‚° ë¹„ìš© í¼  

- **Layer freezing**: conv baseì˜ ê°€ì¤‘ì¹˜ë¥¼ ê³ ì •í•˜ì—¬ ì‚¬ì „í•™ìŠµëœ í‘œí˜„ì„ ë³´ì¡´  

---

## ë¶€ë¶„ íŒŒì¸íŠœë‹
- ëŒ€ê·œëª¨ pretrained model â†’ **ìƒìœ„ ì¼ë¶€ layerë§Œ íŒŒì¸íŠœë‹**  
- ì´ìœ :  
  - í•˜ìœ„ layer = ì¼ë°˜ì ì¸ íŠ¹ì§• (ì—ì§€, ìƒ‰ìƒ, ì§ˆê°) â†’ ì¬ì‚¬ìš© ê°€ì¹˜ ë†’ìŒ  
  - ìƒìœ„ layer = íŠ¹ì • íƒœìŠ¤í¬ íŠ¹í™” íŠ¹ì§• â†’ ìƒˆ ë¬¸ì œì— ë§ê²Œ ì¡°ì • í•„ìš”  
  - íŒŒë¼ë¯¸í„°ê°€ ë§ì„ìˆ˜ë¡ ê³¼ì í•© ìœ„í—˜ â†‘  
- ì˜ˆ: Xception (ì•½ 1,500ë§Œ íŒŒë¼ë¯¸í„°) â†’ ìƒìœ„ layerë§Œ fine-tune  

---

## ë‚´ê°€ ì´í•´í•œ í•µì‹¬
- Dense vs Conv: ì „ì—­ íŒ¨í„´ vs êµ­ì†Œ íŒ¨í„´ í•™ìŠµ  
- CNN: ì´ë™ ë¶ˆë³€ì„± + ê³„ì¸µì  íŠ¹ì§• í•™ìŠµ  
- Padding/Stride: ì¶œë ¥ í¬ê¸° ì¡°ì ˆ  
- Max Pooling: íŠ¹ì§• ì¡´ì¬ ì—¬ë¶€ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì¶”ì¶œ  
- ê³¼ì í•© â†’ ë°ì´í„° ì¦ê°• + Dropout  
- ì „ì´í•™ìŠµ = pretrained conv base ì¬ì‚¬ìš© â†’ ì‘ì€ ë°ì´í„°ì…‹ì— ê°•ë ¥  
- íŠ¹ì§• ì¶”ì¶œ vs íŒŒì¸íŠœë‹: íš¨ìœ¨ì„±ê³¼ ì ì‘ì„±ì˜ trade-off  
- ë¶€ë¶„ íŒŒì¸íŠœë‹: ìƒìœ„ layerë§Œ ì¡°ì •í•´ ê³¼ì í•© ìµœì†Œí™”  

---

## ì½”ë“œ
<details>
<summary>ì½”ë“œ ë³´ê¸°</summary>

```python
### 1. Environment Setup
import os
os.environ["KERAS_BACKEND"] = "jax"

from IPython.core.magic import register_cell_magic

@register_cell_magic
def backend(line, cell):
    current, required = os.environ.get("KERAS_BACKEND", ""), line.split()[-1]
    if current == required:
        get_ipython().run_cell(cell)
    else:
        print(
            f"This cell requires the {required} backend. To run it, change KERAS_BACKEND to "
            f"\"{required}\" at the top of the notebook, restart the runtime, and rerun the notebook."
        )
### 2. Download and Extract Data
import kagglehub

kagglehub.login()
download_path = kagglehub.competition_download("dogs-vs-cats")

import zipfile

with zipfile.ZipFile(download_path + "/train.zip", "r") as zip_ref:
    zip_ref.extractall(".")
### 3. Prepare Dataset (Train / Validation / Test)
import os, shutil, pathlib

original_dir = pathlib.Path("train")
new_base_dir = pathlib.Path("dogs_vs_cats_small")

def make_subset(subset_name, start_index, end_index):
    for category in ("cat", "dog"):
        dir = new_base_dir / subset_name / category
        os.makedirs(dir)
        fnames = [f"{category}.{i}.jpg" for i in range(start_index, end_index)]
        for fname in fnames:
            shutil.copyfile(src=original_dir / fname, dst=dir / fname)

make_subset("train", start_index=0, end_index=1000)
make_subset("validation", start_index=1000, end_index=1500)
make_subset("test", start_index=1500, end_index=2500)
### 4. Load Data
from keras.utils import image_dataset_from_directory

batch_size = 64
image_size = (180, 180)
train_dataset = image_dataset_from_directory(
    new_base_dir / "train", image_size=image_size, batch_size=batch_size
)
validation_dataset = image_dataset_from_directory(
    new_base_dir / "validation", image_size=image_size, batch_size=batch_size
)
test_dataset = image_dataset_from_directory(
    new_base_dir / "test", image_size=image_size, batch_size=batch_size
)
### 5. Data Augmentation
import keras
from keras import layers
import tensorflow as tf

data_augmentation_layers = [
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.2),
]

def data_augmentation(images, targets):
    for layer in data_augmentation_layers:
        images = layer(images)
    return images, targets

augmented_train_dataset = train_dataset.map(
    data_augmentation, num_parallel_calls=8
)
augmented_train_dataset = augmented_train_dataset.prefetch(tf.data.AUTOTUNE)
### 6. Load Pretrained Model (Xception)
import keras_hub

conv_base = keras_hub.models.Backbone.from_preset(
    "xception_41_imagenet",
    trainable=False,
)
preprocessor = keras_hub.layers.ImageConverter.from_preset(
    "xception_41_imagenet",
    image_size=(180, 180),
)
conv_base.trainable = False
len(conv_base.trainable_weights)
### 7. Build Model (Feature Extraction)
inputs = keras.Input(shape=(180, 180, 3))
x = preprocessor(inputs)
x = conv_base(x)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(256)(x)
x = layers.Dropout(0.25)(x)
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs, outputs)
model.compile(
    loss="binary_crossentropy",
    optimizer="adam",
    metrics=["accuracy"],
)
### 8. Train Model (Feature Extraction)
callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath="feature_extraction_with_data_augmentation.keras",
        save_best_only=True,
        monitor="val_loss",
    )
]
history = model.fit(
    augmented_train_dataset,
    epochs=30,
    validation_data=validation_dataset,
    callbacks=callbacks,
)
test_model = keras.models.load_model(
    "feature_extraction_with_data_augmentation.keras"
)
test_loss, test_acc = test_model.evaluate(test_dataset)
print(f"Test accuracy: {test_acc:.3f}")
### 9. Prepare for Fine-Tuning
conv_base.trainable = True

for layer in conv_base.layers[:-4]:
    layer.trainable = False
    
for layer in conv_base.layers:
    if isinstance(layer, layers.BatchNormalization):
        layer.trainable = False
### 10. Train Model (Fine-Tuning)
model.compile(
    loss="binary_crossentropy",
    optimizer=keras.optimizers.Adam(learning_rate=1e-5),
    metrics=["accuracy"],
)

callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath="fine_tuning.keras",
        save_best_only=True,
        monitor="val_loss",
    )
]
history = model.fit(
    augmented_train_dataset,
    epochs=30,
    validation_data=validation_dataset,
    callbacks=callbacks,
)
model = keras.models.load_model("fine_tuning.keras")
test_loss, test_acc = model.evaluate(test_dataset)
print(f"Test accuracy: {test_acc:.3f}")
len(conv_base.trainable_weights)
```
</details> 

---

## ğŸ“– ì›ë¬¸ ë°œì·Œ

<details>
<summary>ì›ë¬¸ ë³´ê¸°</summary>
The fundamental difference between a densely-connected layer and a convolution layer is this: Dense layers learn global patterns in their input feature space (for example, for a MNIST digit, patterns involving all pixels), whereas convolution layers learn local patterns: in the case of images, patterns found in small 2D windows of the inputs. 

This key characteristic gives convnets two interesting properties:
- The patterns they learn are translation invariant.
- They can learn spatial hierarchies of patterns 

That is what the term feature map means: every dimension in the depth axis is a feature (or filter), and the rank-2 tensor output[:, :, n] is the 2D spatial map of the response of this filter over the input.

If you want to get an output feature map with the same spatial dimensions as the input, you can use padding. Padding consists of adding an appropriate number of rows and columns on each side of the input feature map so as to make it possible to fit centered convolution windows around every input tile.

In Conv2D layers, padding is configurable via the padding argument, which takes two values: "valid", which means no padding (only valid window locations will be used); and "same", which means â€œpad in such a way as to have an output with the same width and height as the input.â€ The padding argument defaults to "valid".

The other factor that can influence output size is the notion of strides. The description of convolution so far has assumed that the center tiles of the convolution windows are all contiguous. But the distance between two successive windows is a parameter of the convolution, called its stride, which defaults to 1. Itâ€™s possible to have strided convolutions: convolutions with a stride higher than 1.

Using stride 2 means the width and height of the feature map are downsampled by a factor of 2 (in addition to any changes induced by border effects). Strided convolutions are rarely used in classification models, but they come in handy for some types of models.

Max pooling consists of extracting windows from the input feature maps and outputting the max value of each channel. Itâ€™s conceptually similar to convolution, except that instead of transforming local patches via a learned linear transformation (the convolution kernel), theyâ€™re transformed via a hardcoded max tensor operation. A big difference from convolution is that max pooling is usually done with 2 Ã— 2 windows and stride 2, in order to downsample the feature maps by a factor of 2. On the other hand, convolution is typically done with 3 Ã— 3 windows and no stride (stride 1).

In a nutshell, the reason is that features tend to encode the spatial presence of some pattern or concept over the different tiles of the feature map (hence, the term feature map), and itâ€™s more informative to look at the maximal presence of different features than at their average presence. So the most reasonable subsampling strategy is to first produce dense maps of features (via unstrided convolutions) and then look at the maximal activation of the features over small patches, rather than looking at sparser windows of the inputs (via strided convolutions) or averaging input patches, which could cause you to miss or dilute feature-presence information.

Overfitting is caused by having too few samples to learn from, rendering you unable to train a model that can generalize to new data. Given infinite data, your model would be exposed to every possible aspect of the data distribution at hand: you would never overfit. Data augmentation takes the approach of generating more training data from existing training samples, by augmenting the samples via a number of random transformations that yield believable-looking images. The goal is that at training time, your model will never see the exact same picture twice. This helps expose the model to more aspects of the data and generalize better.

If you train a new model using this data-augmentation configuration, the model will never see the same input twice. But the inputs it sees are still heavily intercorrelated, because they come from a small number of original images â€“ you canâ€™t produce new information, you can only remix existing information. As such, this may not be enough to completely get rid of overfitting. To further fight overfitting, youâ€™ll also add a Dropout layer to your model, right before the densely connected classifier.

A common and highly effective approach to deep learning on small image datasets is to use a pretrained model. A pretrained model is a model that was previously trained on a large dataset, typically on a large-scale image-classification task. If this original dataset is large enough and general enough, then the spatial hierarchy of features learned by the pretrained model can effectively act as a generic model of the visual world, and hence its features can prove useful for many different computer vision problems, even though these new problems may involve completely different classes than those of the original task. For instance, you might train a model on ImageNet (where classes are mostly animals and everyday objects) and then repurpose this trained model for something as remote as identifying furniture items in images. Such portability of learned features across different problems is a key advantage of deep learning compared to many older, shallow-learning approaches, and it makes deep learning very effective for small-data problems.

There are two ways to use a pretrained model: feature extraction and fine-tuning. 

Feature extraction consists of using the representations learned by a previously-trained model to extract interesting features from new samples. These features are then run through a new classifier, which is trained from scratch.

In the case of convnets, feature extraction consists of taking the convolutional base of a previously-trained network, running the new data through it, and training a new classifier on top of the output.

Why only reuse the convolutional base? Could you reuse the densely-connected classifier as well? In general, doing so should be avoided. The reason is that the representations learned by the convolutional base are likely to be more generic and therefore more reusable: the feature maps of a convnet are presence maps of generic concepts over a picture, which is likely to be useful regardless of the computer vision problem at hand. But the representations learned by the classifier will necessarily be specific to the set of classes on which the model was trained â€“ they will only contain information about the presence probability of this or that class in the entire picture. Additionally, representations found in densely connected layers no longer contain any information about where objects are located in the input image: these layers get rid of the notion of space, whereas the object location is still described by convolutional feature maps. For problems where object location matters, densely connected features are largely useless.

Note that the level of generality (and therefore reusability) of the representations extracted by specific convolution layers depends on the depth of the layer in the model. Layers that come earlier in the model extract local, highly generic feature maps (such as visual edges, colors, and textures), whereas layers that are higher up extract more-abstract concepts (such as â€œcat earâ€ or â€œdog eyeâ€). So if your new dataset differs a lot from the dataset on which the original model was trained, you may be better off using only the first few layers of the model to do feature extraction, rather than using the entire convolutional base.

At this point, there are two ways you could proceed:
1. Running the convolutional base over your dataset, recording its output to a NumPy array on disk, and then using this data as input to a standalone, densely-connected classifier similar to those you saw in part 1 of this book. This solution is fast and cheap to run, because it only requires running the convolutional base once for every input image, and the convolutional base is by far the most expensive part of the pipeline. But for the same reason, this technique wonâ€™t allow you to use data augmentation.
2. Extending the model you have (conv_base) by adding Dense layers on top, and running the whole thing end to end on the input data. This will allow you to use data augmentation, because every input image goes through the convolutional base every time itâ€™s seen by the model. But for the same reason, this technique is far more expensive than the first.

Freezing a layer or set of layers means preventing their weights from being updated during training. Here, if you donâ€™t do this, then the representations that were previously learned by the convolutional base will be modified during training. Because the Dense layers on top are randomly initialized, very large weight updates would be propagated through the network, effectively destroying the representations previously learned.

Fine-tuning consists of unfreezing the frozen model base used for feature extraction, and jointly training both the newly added part of the model (in this case, the fully connected classifier) and the base model. This is called fine-tuning because it slightly adjusts the more abstract representations of the model being reused, in order to make them more relevant for the problem at hand.

The steps for fine-tuning a network are as follows:
- Add your custom network on top of an already-trained base network.
- Freeze the base network.
- Train the part you added.
- Unfreeze the base network.
- Jointly train both these layers and the part you added.
Note that you should not unfreeze â€œbatch normalizationâ€ layers (BatchNormalization). 

Partial fine-tuning
In this case, we chose to unfreeze and fine-tune all of the Xception convolutional base. However, when dealing with large pretrained models, you may sometimes only unfreeze some of the top layers of the convolutional base, and leave the lower layers frozen. Youâ€™re probably wondering, why only fine-tune some of the layers? Why the top ones specifically? Hereâ€™s why:
- Earlier layers in the convolutional base encode more-generic, reusable features, whereas layers higher up encode more-specialized features. Itâ€™s more useful to fine-tune the more specialized features, because these are the ones that need to be repurposed on your new problem. There would be fast-decreasing returns in fine-tuning lower layers.
- The more parameters youâ€™re training, the more youâ€™re at risk of overfitting. The convolutional base has 15 million parameters, so it would be risky to attempt to train it on your small dataset.
</details>